---
title: "The Internet Application – Now & In the Future"
date: 2020-03-05T18:11:20+01:00
draft: false
author: Aideen
description: Some 30 years ago, Tim Berners-Lee invented the World Wide Web. Today, we need to future-proof the web prioritizing privacy of personal data and giving power to individual networks.
cover:
tags:
  - kubernetes
  - reliability
  - containers
  - serverless
  - Web3.0
---

In a world of virtual datacentres, containers and serverless computing, I want to begin our exploration of scalable and cloud computing with where it all began, bare metal. By 1995 bare metal servers were ubiquitous and Apache HHTP server is the web server of choice for most engineers. As datacentres emerge as the internet evolves, the term “Cloud Computing”1 is first used in an internal paper by Compaq in 1996. Their vision was detailed and prescient. Not only would all business software move to the Web, but what they termed “cloud computing-enabled applications” like consumer file storage would become common. Once VMWare was launched in 1998, the core principles of cloud computing were solidified, and virtualization gained popularity with the development of modern virtual machines. Virtualization2 catalysed the move from running a single program on a single machine to running multiple operating systems and applications on a single server, enabling resource optimization, maximised uptime and isolation. Gradually machines became networked together and client-server architectures were born to enable a low-powered machine to harness the greater power of a mainframe in another room or building.

This early client server computing model was inconvenient. Every application had its precompiled client program that had to be separately installed on each user’s PC and the client received a web page as a static document making an interactive user experience and updates difficult. When Netscape presented the client-side scripting language in 1995 quickly followed by Macromedia Flash in 1996, the internet became exponentially faster and productive. The concept of a web application emerged with the Java language alongside AJAX empowered web developers to develop asynchronous web apps. AJAX was the first articulation of the Single-Page Application (SPA). With JavaScript rapidly evolving and gaining a reputation for the language to build consumer-grade applications, the open-source community respond with developing diverse libraries, toolkits and frameworks contributing to client-side development through the 2000s. Libraries such as Dojo, Prototype, jQuery and YUI are widely adopted and the number of libraries supporting HTTP, storage, templating, routing and MVC grow. The JavaScript era slowed down with the emergence of Backbone and then Angular JS that set the standard for meeting user experience. HTTP/2 adoption made our applications faster and simpler by focusing on reducing latency and minimizing protocol overhead. Today, JavaScript frameworks are still developing at an extremely fast past with the release of React and Vue.js which are providing the framework to build robust, responsive and highly agile web applications. This brings us to the browsers of today, but where did the internet of today come from and how we will meet the reliability, scalability and security constraints of our future systems?

To answer this, we need to go back to the establishment of the client-server architecture. To meet the rise of the internet and the increase in the size and number of datacentres, Dr Peter Rogers coined the term “micro web services“ and addressed the fundamental need to break applications into smaller pieces for faster software development at a cloud computing conference in 2005. Unlike client-service architectures, this microservice like architecture or distributed systems application is composed of multiple applications running on different machines all communicating together. With the launch of EC2 from AWS one year later, companies, businesses and individuals were now empowered to dynamically scale their applications and run them securely in the cloud. This led to a widespread development of distributed systems, moving developers away from the more monolithic and service-oriented architectures.

Today’s world of always on APIs and applications have availability and reliability requirements that would have been only been required of the world’s most critical services 20 years ago. These applications need to be built so they can scale instantly to support a sudden increase in global demand. These requirements and constraints mean that almost every application needs to be a distributed system. Unfortunately, the benefits of distributed systems come with a price. Distributed systems are significantly more difficult to build, run and debug than single machine applications. Regardless of the difficulty and higher engineering skills required, the need for reliable and scalable systems has no sign of slowing down.

Fortunately, new technologies have emerged to support engineers in developing distributed systems. Containers, container images and container orchestrators have gained massive adoption and popularity, acting as building blocks to empower developers to build distributed systems reliably, efficiently and at scale. Seeing containers as more than just a vehicle of deployment but analogous to objects in object-oriented software, we can use containers and container orchestrators as a foundation to establish reusable design patterns that can make building distributed systems more accessible – no longer excluding the building of distributed systems to the domain of experts but to developers who just need to build applications. Similar to Microsoft’s unique history of enabling developer productivity with the emergence of Visual Basic, Axis database and managed code like .NET, the Cloud Native Computing Foundation(CNCF)3 is acting as an incubator for orchestration tools like Kubernetes, Helm, Prometheus, Envoy and Draft that can be added to every developer’s toolkit. Today, container orchestration is becoming a commodity; developers want to consume container-oriented primitives.

To fully understand the future of the internet and distributed systems, I believe we must spend some more time looking at the operating system of today: Kubernetes. Google was running containers at scale for production workloads long before anybody else. This container orchestration system was called Borg4 and solved the problem of running many services at global scale on millions of servers. In 2014, Google founded the open source project, Kubernetes, based on the lessons they learned form Borg and its successor Omega5. Kubernetes is likely to become so much part of the fabric of computing, that it is likely to disappear and become part of the plumbing, like how virtualization once a novel technology is now just a utility. It was never meant to be the final layer of the stack and the community and CNCF is working for it’s future to lie largely in the realm of managed services. Cloud-native applications like Kubernetes are providing the automation and observability to support companies in becoming more technologically capable and competitive in managing their applications at scale and at high velocity. This is a very exciting time as new applications such as ML , edge computing and IoT find their way into the Cloud native ecosystem via projects such as KubeFlow6.

Interestingly, in some ways cloud functions are more convenient than Kubernetes. Before the concept of Serverless came along and the release of AWS lambda in 2014, developers were forced to spend significant time writing supporting code rather than building the application itself. Building and maintaining applications that easily scale to support spikes in demand or a global user base, has required a large amount of upfront engineering work and operational support. While Containers offer OS level virtualization, Serverless can offer application level virtualization. For example, Cloudflare Workers provides a lightweight JavaScript execution environment that allows developers to augment existing applications or create entirely new ones without configuring or maintaining infrastructure10. Although not all workloads are suitable for FaaS platforms, their cost, compute performance and simplicity make it a great solution amongst developers along with managed servers, horizontal scaling and simple deployments and updates. These function-based apps are replacing microservice style architectures and background type services. However, just like technical cost associated with distributed systems, there are barriers to getting started with these serverless technologies - using different interfaces, a dramatic shift in development methodology, a change in the thought process behind deployments and a dependency on a third-party vendor.

Outside of the public FaaS platforms, like Lambda or Azure, you can run FaaS applications on your Kubernetes Cluster with the help of OpenFaas and other open source projects. This hybrid of functions and containers is sometimes called funtainers. KNative7 is a software delivery platform harnessing the power of both functions and containers - high availability, no-ops, and serverless computing, all running atop containers. Its rise has been meteoric in terms of adoption, the number of open source contributions and emergence of similar projects like IBM OpenWhisk8, OpenFaas and Spotinst9 .This seismic shift in production and development may mean that in the future the distinction between containers and functions may blur or disappear altogether.

The widespread deployment of IoT devices, 5G network, machine learning, AR/VR facilities and Big Data infrastructures is driving enterprises to shift their intentions from the centralized computing architecture to a distributed edge approach (According to Gartner, by 2025 75% of enterprise-generated data will be created and processed outside a traditional data centre12).These “smart” deployments generate massive amounts of data and require significant network bandwidth for edge generated data to travel to the central cloud for analytics, increasing latency and in the process potentially harming your business Service Level Agreements and Objectives. Therefore, it makes sense to do the computing where the data comes from. This allows companies to solve production bottlenecks and do predictive maintenance(In many ways, Content Delivery Networks like Cloudflare were the precursor to the Edge Cloud.) Facing this transition to the 5G and IoT, network operators will have to consider the hardware capability of edge servers, such as processor, memory and storage and even graphic processing to perform data analytics. An edge cloud should also use virtualization, workload segmentation, automation and orchestration of activities, and the ability to distribute workloads from one edge to another.

In our modern web’s client server model, the server will always occupy a central role in the communication between client-server creating a centralized model of web services. Since this server occupies this critical position, it is a single point of failure and attack. Beyond the physical limits to this client-server model, the consolidation of web hosting providers is a higher-level problem where our data and content is hostage to the servers of a few mega companies. This leads to problems ranging from censorship at the behest of national governments to more subtle, perhaps even unintentional, bias in the curation of content users see based on opaque, unaudited curation algorithms. Although today’s mega-platforms are built on top of the Web’s distributed and open, protocols, there is an inherent bias towards market consolidation and because user data is important for monetizing these platforms, there is little push form these mega-platforms to adopt alternative protocols when owning data is most profitable.

In the advent of Web 3.0, the decentralized web focuses on a peer to peer architecture where services are distributed rather than localized, where users have more control over their data with no reliance on intermediaries to connect us. Projects like Mastodon13, Beaker14 browser, Blockstack15 and the Ethereum network are all helping to build the decentralized web of today along with data storage projects like IPFS16, FileCoin17 and Storj18, which are providing the critical infrastructure for decentralized apps. However, most of these systems like IPFS haven’t solve show to manage private cryptographic keys and grasp a model of complex security protocols. Web 3.0 models the underlying concepts of BitTorrent and Gnutella, however there is reason these haven’t become the de facto way of downloading information as they are hard to maintain and manage contestation ratios.

Large centralized systems are optimized for serving dynamic content and database consistency. In every distributed system you must balance throughput, latency, availability and consistency. The PACELC theorem is an extension of the CAP theorem and states that in case of network partitioning (P) in a distributed computer system, one has to choose between availability (A) and consistency (C) but else (E), even when the system is running normally in the absence of partitions, one has to choose between latency (L) and consistency (C)19. CRDT20 or Conflict-free replicated data type is a solution to the partition consistency problem in decentralized systems. Here write-consistency becomes less of problem but meeting global read consistency is almost impossible, and even if consistency is achieved, latency would be off the charts. Bitcoin21 is a great case study of this. Being a decentralized transaction exchange, it must be consistent. Bitcoin tried to balance consistency against latency using generations. The result is that latency is high, but throughput is very low. Can such a model replace the likes of Mastercard or Visa? Centralized systems have economies of scale by centralizing their work and minimising the energy needed. On the other hand, decentralized systems operating at the same scale with our current hardware, consume much more energy than their centralized counterparts contributing more negatively to the environment.

As we talk about what the future of the internet and internet applications will look like, a quote by the futurist Irwin Corey lives at the forefront of my mind:
->_If we don’t change direction now, we’ll end up where we’re going_<-
Some 30 years ago, Tim Berners-Lee invented the World Wide Web. Today, we need to future-proof the web prioritizing privacy of personal data and giving power to individual networks. No matter what, one thing is for certain and that’s the demand for our systems to be reliable, scalable, efficient and secure is only going to grow. For me, the future of internet applications lies in the orchestration tools and pattens that will run ensure and enable the delivery, maintenance and management of these systems.

## References

[1] https://www.ibm.com/blogs/cloud-computing/2014/03/18/a-brief-history-of-cloud-computing-3/
[2] https://www.ibm.com/it-infrastructure/z/capabilities/virtualization
[3] https://kuberneteslaunch.com/
[4] https://pdos.csail.mit.edu/6.824/papers/borg.pdf
[5] https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41684.pdf
[6] https://www.kubeflow.org/
[7] https://cloud.google.com/knative/
[8] https://spotinst.com/
[9] https://developer.ibm.com/open/projects/openwhisk/
[10] https://workers.cloudflare.com/
[11] https://www.cloudflare.com/learning/cdn/what-is-a-cdn/
[12] https://www.gartner.com/smarterwithgartner/what-edge-computing-means-for-infrastructure-andoperations-leaders/
[13] https://mastodon.social/about
[14] https://beakerbrowser.com/
[15] https://blockstack.org/
[16] https://ipfs.io/
[17] https://filecoin.io/
[18] https://storj.io/
[19] https://en.wikipedia.org/wiki/PACELC_theorem
[20] https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type
[21] https://bitcoin.org/en/
